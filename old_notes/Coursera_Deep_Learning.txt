Deep Learning
    5-course Specialization

COURSE 1
    Neural Networks and Deep Learning
    Upcoming session: Sep 12 — Oct 16.
    4 weeks of study, 3-6 hours a week    
    https://www.coursera.org/learn/neural-networks-deep-learning/discussions
        WEEK 1
            Introduction to deep learning
            Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.
            7 videos, 2 readings
            Build a Cat Recognizer
            Build a Cat Recognizer
		Discussion Forum:
        WEEK 2
            Neural Networks Basics
            Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.
            19 videos, 1 reading
		Programming Exercies
        WEEK 3
            Shallow neural networks (Single Hidden layer)
            Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.
            12 videos
        WEEK 4
            Deep Neural Networks
            Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.
            8 videos

COURSE 2
    Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
    Upcoming session: Sep 12 — Oct 9.
    3 weeks, 3-6 hours per week 

COURSE 3
    Structuring Machine Learning Projects
    Upcoming session: Sep 12 — Oct 2.
    2 weeks of study, 3-4 hours/week


COURSE 4
    Convolutional Neural Networks
        for Images

COURSE 5
    Sequence Models 
        for NLP
        RNN and LSTM
===========================================================================================================================================================
RELU Rectified Linear Unit        

Types
    Standard NN  (Regression)
    Convolutional NN (Images)    
    Recrrent NN (Language/Audio)
Supervised Learning
    Structured Data
    UnStructured Data
        Audio
        Images
        Text 
Why now
	- Amount of Data -- but performace of MC Learning does not improve with data size
	- Progress in fast computing and GPU
	- RELU
	- Idea -> Code -> Experiment [[ Train/test Cycle :: 10 minutes v/s for a month ]]

6 of 7 vidoes watched    

Week2 -- Aug 22
	Binary Classification
			m = Training set size
			forward propagation step and backward propation step
		Logistic Regression
			Problem ( Input = cat ) (output = y = cat / non cat)
			RGB matrices 64x64
			Input feature vector X into 64x64x3 = 12288
		Notations
			(x,y) = single training example ( y== 0 or 1)
			m = size of training set
			X= .. m =colums (number of rows = number of data elements in each input matrix)
				x by m dimentional Matrix (x,m)
			Y = 1 by m dimentional Matrix (1,m)
	Logistic Regression
		Given X find y^ ... y^ = P (y=1|x)
		sigmoid(z) == 1/1 + e^-z  .. o to 1 [[[ z = w transpose x + b ]]]
				z == wx + b
		w .... nx dimentional vector
		b .... real number
		
	Logistic Regression Cost Function
		We learnt
			y^=sigmoid( w transpose x + b))
			x(1),y(1) ... to x(m),y(m)
		Loss (error function) = to measure how good is y^ if the true output is y
			** Need a convex function // not multiple local minimums) for gradient descent to work well **
			L(y^, y) = -(y log y^ + (1-y)log(1-y^) )
			we want loss  to be as low as possible
				y = 1 ==== y^ .. as big   as possible(1 is the max of sigmoid(x))
				y = 0 ==== y^ .. as small as possible(0 is the min of sigmoid(x))
		Cost Function 
			J(w,b) 	=   1/m (sum(i=1 to m) L(y^(i), y(i))
				= - 1/m (sum(i=1 to m)(y(i) log y(i)^ + (1-y(i))log(1-y(i)^) )
	Gradient Descent
		We Learnt
			y^=sigmoid( w transpose x + b))
                    sigmoid(z) == 1/(1 + e^-z)  .. o to 1 [[[ z = w transpose x + b ]]]
                    x(1),y(1) ... to x(m),y(m)
			Cost Function (target of cost function is to be as small as possible)
			J(w,b) 	=   1/m (sum(i=1 to m) L(y^(i), y(i))
				= - 1/m (sum(i=1 to m)(y(i) log y(i)^ + (1-y(i))log(1-y(i)^) )
            w := w - alpha (partial derivative J(w,b)/partial derivative(w))    ||| slope in the direction of b
            b := b - alpha (partial derivative J(w,b)/partial derivative(b))    ||| slope in the direction of b
            
            w := w - alpha (d(w))
            b := b - alpha (d(b))
	Derivatives
        f(a)=3a
        Slope =  
            derivative of a at a=2 is 0.003/0.001 = 3
            derivative of a at a=5 is 0.003/0.001 = 3
            d(f(a))/d(a) = 3 
            
    Derivatives with Computation Graph
        Forward Pass and Back prop
        J(a,b,c) = 3(  a +  bc  )
                          u=bc
                     v=a + u
                J=3  v
        a->v->J         read as a impacts v and v impacts J
        derivative(J)/derivative(v)=3  ==== dv=3
        derivative(v)/derivative(a)=1  
        derivative(J)/derivative(a) = product = derivative(J)/derivative(v) * derivative(v)/derivative(a)  = 3    ===== da = 3
        "dvar" = will repesent the derivative of J with respect to some variable var
        du = 3
        db = 6
        dc = 9
    Logistic Regression Gradient Descent
    Python and Vectorization: Vectorizaiton
                import numpy as np
                a=np.array([1,2,3,4])
                print(a)
                ....
                import time
                a = np.random.rand(1000000)
                b = np.random.rand(1000000)
                tic = time.time()
                c = np.dot(a,b)
                toc = time.time()
                print (str(1000*(toc-tic)) + "milli seconds")
                print (c)
                ....
                d=0
                tic = time.time()
                for i in range(1000000):
                    d += a[i]*b[i]

                toc = time.time()
                print (str(1000*(toc-tic)) + "milli seconds")
                print(d)
    Python and Vectorization: Vectorizing Logistic Regression
    Python and Vectorization: Broadcasting
        import numpy as np
        A = 3x4 matrix 3 rows 4 columns
        A = np.array ([[56.0, 0.0, 4.4, 68.0],
                       [1.2,  104, 52,  8.0],
                       [1.8,  135, 99,  0.9]])
        cal=np.sum(A,axis=0)
        pct = 100*A/cal.reshape(1,4)
        print pct
        ...........
        A=np.array([1,2,3,4])
        print (A.reshape(4,1)+100)
        print (A.reshape(1,4)+100)
        ...........
        A=np.array([[1,2,3],
                    [4,5,6]])
        B=np.array([[100, 200, 300]]).reshape(1,3)
        B=np.array([[100, 200, 300]])
        print (A+B)
        .....
        A=np.array([[1,2,3],
                    [4,5,6]])
        B=np.array([[100, 200]]).reshape(2,1)
        print (A+B)
        .............
        a= np.random.randn(5)
        print(a.shape)
        b=a.reshape(5,1)
        print(b.shape)
        print(a.T)
        print(b.T)
        print(np.dot(a,a.T))
        print(np.dot(b,b.T))
        -- USE -- 
        a= np.random.randn(5,1)  ### Column Vector 
        a= np.random.randn(1,5)  ### Row Vector
    Python and Vectorization: Logistic Regression Cost Function
    Week2 : Python Basics with Numpy v3  -- Exercises
    import math
    
    def basic_sigmoid(x):
        s = 1 /(1 + math.exp(-1*x))
        return s

    def sigmoid_derivative(x):
        s = 1 /(1 + np.exp(-1*x))
        ds = s*(1-s)
        return ds

    def image2vector(image):
        x = image.reshape((image.shape[0]*image.shape[1], image.shape[2]))
        v = x.reshape(x.shape[0]*x.shape[1],1)
        return v        
    .....
    x = np.array([[0,3,4],[2,6,4]])
    .....
    n = np.linalg.norm(x, axix=1, keepdims = true)
    x_normalized = x / np.linalg.norm(x, axis=1, keepdims = True)
    
    def normalizeRows(x):
        import numpy as np
        x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims = True)
        x = x / x_norm
        return x
    ..........
        np.sum(x, axis=0) ## sum all the columns [[ m x n => 1 x n]]
        np.sum(x, axis=1) ## sum all the rows    [[ m x n => 1 x m]]
    def softmax(x):
        x_exp = np.exp(x)  
        x_sum = np.sum(x_exp, axis=1, keepdims = True)
        s = x_exp / x_sum
        return s
    .................
    def L1(yhat, y):
        loss = sum(abs(yhat -y))
        return loss
    ................
    def L2(yhat, y):
        loss = sum((np.square(y-yhat)))
        return loss
    ..........................................................
    Logistic Regression with a Neural Network mindset
    ..........................................................
    anaconda3/bin/jupyter-console
    import numpy as np
def initialize_with_zeros(dim):
    w = np.zeros((dim,1))
    b = 0
    return w, b
    
def sigmoid(z):
        s =  1/ ( 1 + np.exp((-1)*(z)) )
        return s
    
def propagate(w, b, X, Y):
        m = X.shape[1]
        A = sigmoid ( np.dot(w.T, X) + b  )                       # compute activation
        cost = np.sum(( Y*np.log(A) + (1 - Y)*np.log(1-A)  )/(-1*m))                       # compute cost
        dw = np.dot(X, ((A - Y).T))/m  
        db = np.sum(A - Y)/m
        grads = {"dw": dw,"db": db}
        return grads, cost
    
    w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])
    
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    costs = []
    m = X.shape[1]
    grads = {}
    for i in range(num_iterations):
        A = sigmoid ( np.dot(w.T, X) + b  )
        grads["dw"] = np.dot(X, ((A - Y).T))/m  
        grads["db"] = np.sum(A - Y)/m
        cost = np.sum(( Y*np.log(A) + (1 - Y)*np.log(1-A)  )/(-1*m))
        dw = grads["dw"]
        db = grads["db"]
        w = w - (learning_rate * dw)
        b = b - (learning_rate * db)
        if i % 100 == 0:
            costs.append(cost)
        params = {"w": w,"b": b}
        grads = {"dw": dw, "db": db}
    return params, grads, costs
    
    params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)
    
def predict(w, b, X):
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)
    A = sigmoid ( np.dot(w.T, X) + b  )  
    for i in range(A.shape[1]):
        if A[0,i] > 0.5:
            Y_prediction[0,i] = 1
    return Y_prediction
    
    ..........................................................
    ..........................................................
Week 3  - Single Hiddnen Layer
Initialize:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
n_x=2
n_h=4
n_y=1
    W1 = np.random.randn(n_h,n_x)*0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h)*0.01
    b2 = np.zeros((n_y,1))
    W1 = np.array ([[-0.00416758, -0.00056267],[-0.02136196,  0.01640271],[-0.01793436, -0.00841747],[ 0.00502881, -0.01245288]])
    W2 = np.array ([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]])
Forward Pass 
X=np.array([[ 1.62434536, -0.61175641, -0.52817175],[-1.07296862,  0.86540763, -2.3015387 ]])
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh (Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid (Z2)
Calculate Cost
Y=np.array([[ 1.62434536, -0.61175641, -0.52817175]])
    m = Y.shape[1] 
    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))
    cost = -(np.sum(logprobs))/m

Backword Propagation
    dZ2= A2-Y
    dW2 = np.dot(dZ2,(A1.T))/m
    db2 = np.sum(dZ2,axis=1,keepdims=True)/m
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = np.dot(dZ1,(X.T))/m
    db1 = np.sum(dZ1,axis=1,keepdims=True)/m        

Apply Lerning Rate
===========================================================================================================================================================
Week 4 - Exercise 1 
===========================================================================================================================================================
Initialize - 2 layer
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    np.random.seed(1)
    n_x, n_h, n_y = 3,2,1
    W1 = np.random.randn(n_h,n_x)*0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(1,n_h)*0.01
    b2 = np.zeros((1,1))    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}

def initialize_parameters_deep(layer_dims):
    np.random.seed(3)
    parameters = {}
layer_dims=[5,4,3]
L = len(layer_dims)           
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

linear_forward(A, W, b):
        Z = np.dot(W,A) + b

def sigmoid(z):
        s =  1/ ( 1 + np.exp((-1)*(z)) )
        return s
        
def linear_activation_forward(A_prev, W, b, activation):
    if activation == "sigmoid":
        Z, linear_cache = np.dot(W,A_prev)+b , (A_prev, W, b)
        A, activation_cache = sigmoid(Z)
    elif activation == "relu":
        Z, linear_cache = np.dot(W,A_prev)+b, (W, b)
        A, activation_cache = relu(Z)

def L_model_forward(X, parameters):
    for l in range(1, L):
        A_prev = A 
        A, cache = relu(np.dot(parameters['W'+str(l)],A_prev) + parameters['b'+str(l)])
        caches.append(cache)
    
    AL, cache = sigmoid(np.dot(parameters['W'+str(L)],A) + parameters['b'+str(L)])
    caches.append(cache)


Z=np.array([[ 3.43896131, -2.08938436]])


X=np.array([[ 1.62434536, -0.61175641], [-0.52817175, -1.07296862], [ 0.86540763, -2.3015387 ], [ 1.74481176, -0.7612069 ]])
parameters={}
parameters["b1"]=np.array([[-1.10061918],[ 1.14472371],[ 0.90159072]])
parameters["b2"]=np.array([[-0.12289023]])
parameters["W1"]=np.array([[ 0.3190391 , -0.24937038,  1.46210794, -2.06014071],[-0.3224172 , -0.38405435,  1.13376944, -1.09989127],[-0.17242821, -0.87785842,  0.04221375,  0.58281521]])
parameters["W2"]=np.array([[ 0.50249434,  0.90085595, -0.68372786]])

Cost Function
AL=np.array([[ 0.8, 0.9, 0.4]])
Y=np.array([[1,1,1]])
     cost = np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),np.log(1-AL)))/(-1*m)

linear_backward
dZ=np.array([[ 1.62434536 -0.61175641]])
cache = []
cache.append(np.array([[-0.52817175, -1.07296862],[ 0.86540763, -2.3015387 ],[ 1.74481176, -0.7612069 ]]))
cache.append(np.array([[ 0.3190391 , -0.24937038,  1.46210794]]))
cache.append(np.array([[-2.06014071]]))
    print("dZ="+str(dZ))
    print("cache="+str(cache))
    
    dW = (np.dot(dZ,A_prev.T))/m
    db = (np.sum(dZ,axis=1,keepdims=True))/m
    dA_prev = np.dot(W.T,dZ)


linear_activation_backward

dA=np.array([[-0.41675785,-0.05626683]])
activation_cache=np.array([[ 0.04153939, -1.11792545]])
A_prev=np.array([[-2.1361961 ,  1.64027081],[-1.79343559, -0.84174737],[ 0.50288142, -1.24528809]])
dZ=np.array([[ 0.04340305,  0.00058787]])

    if activation == "relu":
        A_prev, W, b = linear_cache
        m = A_prev.shape[1]
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = np.dot(W.T,dZ), np.dot(dZ,A_prev.T)/m, np.sum(dZ,axis=1,keepdims=True)/m
        
    elif activation == "sigmoid":
        A_prev, W, b = linear_cache
        m = A_prev.shape[1]
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = np.dot(W.T,dZ), np.dot(dZ,A_prev.T)/m, np.sum(dZ,axis=1,keepdims=True)/m

def L_model_backward(AL, Y, caches):
caches=(((array([[ 0.09649747, -1.8634927 ],
       [-0.2773882 , -0.35475898],
       [-0.08274148, -0.62700068],
       [-0.04381817, -0.47721803]]), array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],
       [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],
       [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]), array([[ 1.48614836],
       [ 0.23671627],
       [-1.02378514]])), array([[-0.7129932 ,  0.62524497],
       [-0.16051336, -0.76883635],
       [-0.23003072,  0.74505627]])), ((array([[ 1.97611078, -1.24412333],
       [-0.62641691, -0.80376609],
       [-2.41908317, -0.92379202]]), array([[-1.02387576,  1.12397796, -0.13191423]]), array([[-1.62328545]])), array([[ 0.64667545, -0.35627076]])))
       
1current_cache=((array([[ 1.97611078, -1.24412333],
       [-0.62641691, -0.80376609],
       [-2.41908317, -0.92379202]]), array([[-1.02387576,  1.12397796, -0.13191423]]), array([[-1.62328545]])), array([[ 0.64667545, -0.35627076]]))
0current_cache=((array([[ 0.09649747, -1.8634927 ],
       [-0.2773882 , -0.35475898],
       [-0.08274148, -0.62700068],
       [-0.04381817, -0.47721803]]), array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],
       [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],
       [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]), array([[ 1.48614836],
       [ 0.23671627],
       [-1.02378514]])), array([[-0.7129932 ,  0.62524497],
       [-0.16051336, -0.76883635],
       [-0.23003072,  0.74505627]]))

===========================================================================================================================================================
===========================================================================================================================================================
